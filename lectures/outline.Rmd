---
title: | 
  | Prospective Outline
  | Empirical Economics
  | 2025-2026
author: |
  | Bas Machielsen
  | a.h.machielsen@uu.nl
date: September 2025
urlcolor: purple
linkcolor: purple
output: 
  pdf_document:
    includes:
      in_header: "preamble.tex"
---

## Prospective Outline Empirical Economics

### Lecture 1: Statistics & Probability

1. Basic Probability Theory:
Experiments, Outcomes, Sample Spaces, Events.

2. Conditional Probability: $P(A|B) = P(A \cap B) / P(B)$.

3. Random Variables:
Definition: A variable whose value is a numerical outcome of a random phenomenon.
Discrete Random Variables vs. Continuous Random Variables.

4. Probability Distributions for Discrete Random Variables:
Probability Mass Function (PMF).
Expected Value ($E[X]$) or Mean of a discrete random variable.
Variance ($Var(X)$) and Standard Deviation of a discrete random variable.
Example with Bernoulli Distribution.

5. Probability Distributions for Continuous Random Variables:
6.1 Probability Density Function (PDF) – area under the curve represents probability.
6.2 Cumulative Distribution Function (CDF) for both discrete and continuous variables.

6The Normal Distribution:
Properties: Bell-shaped, symmetric, defined by mean and variance
The Standard Normal Distribution (Z-distribution)
Using Z-tables or software to find probabilities.

7. Covariance and Correlation:
Covariance: Measuring the direction of linear relationship.
Correlation Coefficient (ρ or r): Measuring strength and direction of linear relationship (-1 to +1).
Distinction: Correlation does not imply causation.

8. Introduction to Sampling:
Population vs. Sample.
Parameters (population characteristics) vs. Statistics (sample characteristics).
Simple Random Sampling.

9. The Concept of a Sampling Distribution:
The distribution of a sample statistic (e.g., sample mean) if we were to draw many samples.

10. The Central Limit Theorem (CLT):
Statement and profound implications for the sampling distribution of the sample mean.
Importance for inference even when the population is not normal (for large enough sample sizes).

11. Introduction to Estimation:
Point Estimators (e.g., sample mean ̄$\bar{x}$ as an estimator for population mean $\mu$.
Desirable properties of estimators (unbiasedness, efficiency, consistency - conceptual).

12. Introduction to Hypothesis Testing (Conceptual):
Null Hypothesis $H_0$ and Alternative Hypothesis $H_A$
The idea of test statistics and p-values (without formal calculations yet).

### Lecture 2: The Linear Model

1. What is Econometrics? Why study it?
The nature of economic data: Cross-sectional, Time Series, Pooled Cross-sections, Panel Data.
The concept of a model: Population Regression Function (PRF).

2. The Sample Regression Function (SRF).
2.1 Derivation of Ordinary Least Squares (OLS) estimators for SLR (minimizing Sum of Squared Residuals - SSR).
2.2 Algebraic properties of OLS statistics (fitted values, residuals).
2.3 Interpreting the intercept $\beta_0$ and $\beta_1$ coefficients in OLS
2.4 Units of measurement and functional form (level-level).
2.5 Goodness-of-Fit: R-squared and Standard Error of the Regression (SER).
2.6 Understanding Statistical Output

3. Classical Assumptions
3.1 No perfect collinearity.
3.2 Zero conditional mean of errors $E(u|x) = 0$ for unbiasedness
3.3 Unbiasedness of OLS estimators
3.4 Variance of OLS estimators 

4. Introduction to Multiple Linear Regression 
4.1 OLS estimation of the MLR model

### Lecture 3: Time Series and Prediction


### Lecture 4: Panel Data

What is Panel Data? Structure ($N$ individuals/entities, $T$ time periods).
Advantages of Panel Data: Controlling for unobserved heterogeneity, increased degrees of freedom, dynamics.
Notation for panel data models.
The Pooled OLS Model on panel data: Assumptions and strong limitations (ignores heterogeneity).
Unobserved Heterogeneity: Individual-specific effects ($\alpha_i$) and time effects ($\lambda_t$).
The Fixed Effects (FE) Model: Treating $\alpha_i$ as parameters to be estimated.
The "Within" Estimator: Transformation via de-meaning data.
The Least Squares Dummy Variable (LSDV) Estimator: Equivalence to Within estimator.
Interpretation of coefficients in FE models (effect of X on Y within individuals over time).
Pros and Cons of FE: Controls for time-invariant unobserved heterogeneity correlated with X, but cannot estimate effects of time-invariant variables.

The Random Effects (RE) Model: Treating $\alpha_i$ as part of the composite error term.
Key Assumption for RE: $E(\alpha_i|X_{it}) = 0$ (unobserved heterogeneity is uncorrelated with regressors).
Estimation of RE models (Generalized Least Squares - GLS, or Feasible GLS - FGLS).
Interpretation of coefficients in RE models.
Pros of RE: Can estimate effects of time-invariant variables, more efficient than FE if assumptions hold.
Cons of RE: Biased if key assumption$E(\alpha_i|X_{it}) = 0$ is violated.
Comparing FE and RE: The Hausman Test - intuition and application.
Null hypothesis of Hausman test (RE is consistent and efficient) and decision rule.
First Differences (FD) Estimator: Another way to eliminate fixed effects; comparison with FE.
Practical considerations: Balanced vs. unbalanced panels, choosing between Pooled OLS, FE, and RE.

### Lecture 5: Binary Outcome

Introduction to Binary (Dummy) Dependent Variables (e.g., employment, purchase decision).
The Linear Probability Model (LPM): Applying OLS to a binary dependent variable.
Interpretation of coefficients in LPM as changes in probability.
Advantages of LPM: Simplicity of estimation and interpretation.
Problem 1 with LPM: Predicted probabilities can be outside $[0,1]$.
Problem 2 with LPM: Non-normality of the error term.
Problem 3 with LPM: Inherent heteroskedasticity of the error term.
The need for non-linear models for binary outcomes.
Introduction to the Latent Variable framework as motivation for Probit/Logit.
The Probit Model: Assumption of a normally distributed latent error term; use of the Normal CDF.
The Logit Model: Assumption of a logistically distributed latent error term; use of the Logistic CDF.
Estimation: Maximum Likelihood Estimation (MLE) – intuition and basic principles.
Interpretation of Coefficients: Raw coefficients are not directly marginal effects.
Calculating and interpreting Marginal Effects (MEs): Average Marginal Effects (AMEs) vs. Marginal Effects at Means (MEMs).
Odds Ratios: Specific interpretation for Logit models.
Goodness-of-Fit for Probit/Logit: Pseudo R-squared (e.g., McFadden's), percent correctly predicted, likelihood ratio tests.
Choosing between Probit and Logit (often similar results, logistic distribution has fatter tails).

### Lecture 6: Difference-in-differences

Recap: Correlation vs. Causation – the fundamental challenge.
The Potential Outcomes Framework (Rubin Causal Model): $Y_i (1), Y_i (0)$
The concept of the Average Treatment Effect (ATE).
The Fundamental Problem of Causal Inference: We only observe one potential outcome per unit.
Selection Bias: Why simple comparisons of treated and untreated groups can be misleading.
Introduction to Differences-in-Differences (DiD): Intuition and basic idea.
The 2x2 DiD Setup: Two groups (Treatment, Control) and two time periods (Before, After).
Calculating the simple DiD estimator: $(\hat{Y}_{T,Post} - \hat{Y}_{T, Pre}) - (\hat{Y}_{C, Post} - \hat{Y}_{C,Pre})$.
The "Parallel Trends" Assumption: The crucial identifying assumption for DiD.
Graphical illustration of the parallel trends assumption and the DiD estimate.
DiD using a Regression Framework: $Y_{it} = \beta_0 + \beta_1 Treat_i + \beta_2 Post_t + β_3 (Treat_i \times Post_t) + \epsilon_{it}$
Interpretation of coefficients in the DiD regression
Advantages of the regression framework (SEs, inclusion of covariates).
Adding Covariates to the DiD model: Controlling for observable differences that might affect trends.
Testing the Parallel Trends Assumption: Pre-treatment trend checks (visual, statistical tests on pre-treatment periods if data allows).
DiD with multiple time periods and staggered adoption (conceptual overview).
Event Study plots as a common way to visualize DiD with multiple periods.
Potential Pitfalls and Limitations of DiD (e.g., Ashenfelter's dip, policy anticipation, spillover effects).
Robustness Checks for DiD studies (e.g., alternative control groups, placebo tests).

### Lecture 7: Propensity Score Matching, IV



### Lecture 8: Regression Discontinuity


